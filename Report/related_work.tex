% Peter %
\subsection{Peter Andersson}
The authors in article~\cite{DBLP:journals/corr/ZhangL15} describes the use of ConvNets (\textit{Convolutional neural Networks}) to understand text. They say that ConvNets can understand text, without the knowledge of words, meaning that the network can extract words from characters. One problem with text understanding is the lack of data sets. Compared to image recognition, there is no large-scale data set available to train a network to understand text. However, the data sets available focuses on certain topics, such as sports, health and computers, etc. They show good results to understand natural language with the use of convolutional networks.
\newline
Article~\cite{lalwani2018implementation} explains the use of a chatbot system using AI and NLP (\textit{Natural Language Processing}). The authors have developed a system to answer questions related to a certain topic (in this case a college). They used a pre-defined question set with corresponding answers in the data base. The chatbot analyses the queries to answer them correctly. A problem in NLP is to understand the questions, not only to provide relevant answers, but also that several questions have the same sense. For example:\\

Q1: In which classrooms will the lectures be held in the course Intelligent Systems?

Q2: Where should I show up to be teached in the course Intelligent Systems?\\

As one can see, the questions Q1 and Q2 both asks for the same thing, but in different kind of words. To solve this problem, the authors developed an algorithm to search for similarities between the question asked and a pre-defined question set. Every question in the question set will be given a score for how much it match the query asked. The question with the highest score will then win and the corresponding answer will be given as output to the user.

Article~\cite{conneau2016very} use very deep ConvNets for their NLP model. They show that a ConvNet with a depth of 29 improves the accuracy, compared to a networks with less depth. Furthermore they explain that a network that is too deep starts to decrease the accuracy. They say that a even deeper network than 29 layers may need some configuration in order to improve the accuracy. There are methods for this, such as shortcuts in the network. However, they did not manage to improve the accuracy of such deep networks, compared to the network with 29 layers.
% Rasmus %
\subsection{Rasmus Södergren}
NLP have many challenges for the machine, where understand text is among the hardest ones. In previous work several authors use deep learning approaches for textual understanding and Question Answering (QA). The authors in \cite{DBLP:journals/corr/YinES16} employs a hierarchical attention-based convolutional neural network(HABCNN). The attention mechanism is used to detect key phrases, key sentences and key snippets that might be relevant to answering a question. In this way they don't manually need to design any features which can be a time consuming and quite hard job otherwise. The authors of \cite{DBLP:journals/corr/YuHBP14} do as well use an approach which allows for no prior to run-time feature generation. They use semantic encoding and a CNN to pair a question with an answer.     
Due to that the feature set doesn't need to be manually generated, it means that the model can be applied to other languages more easily.   
In the paper by Wen-tau Yih et al.\cite{yih2014semantic} they demonstrate a successful semantic parsing convolutional neural network approach for open-domain QA. With help from a knowledge bank (KB) and the CNN they compare the similarity of entity mentions and relation patterns in the question with the KB. The top scoring relations will answer the question.
There are a good idea to try to filter the question given by identifying the relevant parts of it. In \cite{DBLP:journals/corr/YuHBP14} they filter out the stop word from the sentence. E.g. 
\begin{itemize}
    \item "This is an example showing off stop word filtration."
    \item "['This', 'example', 'showing', 'stop', 'word', 'filtration', '.']"
\end{itemize}
This example shows how the relevant parts of the sentence is still there and there are now less data to process.  
% Staffan %0
\subsection{Staffan Brickman}
\textbf{Ask Me Anything: Dynamic Memory Networks for Natural Language Processing (2015)}
\newline
The paper presents a general approach to natural language processing called Dynamic Memory Network (DMN). The DMN is specialized on answering generic questions on a given subject and use an episodic memory module to do it. The DMN is decided into four parts:
\begin{itemize}
\item \textbf{Input Module} \newline This module is feed with text about the subject the network should know something about. Like a movie review, a news article or a Wikipedia page. This is the base “knowledge” of the network. This module translates the input text into distributed vectors for used in the network
\item \textbf{Question Module} \newline This modul is feed the question being answered by the network and translate the input into a distributed vector.
\item \textbf{Episodic Memory Module} \newline This module have an attention mechanic. This means that it decides on what input representation to focus on (from module 1). The module iterates over the inputs to find answers for a given question.
\item \textbf{Answer Module} \newline Generate the answer to the question.
\end{itemize}
\textbf{NLTK: The Natural Language Toolkit (2002)}
\newline
This paper describes the The Natural Language Toolkit for python. It describes the structure and use of the toolkit and also provides some examples. They claim that the toolkit is a good match for almost anyone interested in natural language processing. For students doing assignments, demonstrations of NLP and also for projects.
The toolkit is implemented in a collection of modules. The core modules is a collection of classes and core data types that the rest of the modules depends on. For example the Token class is implemented here and used through out the toolkit. The rest of the modules is called task modules. Each module focuses on a given NLP task, for example input processing. They can be used independently or together as needed. One task module given extra focus in the paper is the corpus module. This module contains large quantities  of written text for training, spellchecking or other use. The module is versatile and contains functions, methods and technics for use in many different situations. Most notably training networks. 
In the processing modules different algorithms are implement as classes. This is, according to the authors, good in different ways. NLTK includes the following modules: cfg, corpus, draw (cfg, chart, corpus, featurestruct, fsa, graph, plot, rdparser, srparser, tree), eval, featurestruct, parser (chart, chunk, probabilistic), probability, sense, set, stemmer (porter), tagger, test, token, tokenizer, tree, and util. 
The rest of the paper focuses on how to install the toolkit, a section on documentation and how the toolkit can be used.
\textbf{Advances in natural language processing (2015)}
\newline
This article published in Science Magazine focuses on the current state of natural language processing. The first part of the article describes what NLP is and why it is more relevant today than ever before. Here it claims that a central advance in NLP is the use of context. Where a given word can mean different things depending on the context. The article goes on describing the subject machine translation and spoken language processing.
Machine reading is an important application of NLP. Today we have vast quantities of written text, but we need to isolate the important parts. This can be used by NLP.  In this way one can gather all available information on a subject without reading millions of pages. NLP used in this maner can be a great tool in any field of research. Another application of NLP mentioned in the article is mining of social media. This is important and has revolutionized, they claim, the ways advertisers, journalists, politicians, and many more acquire their data. 
Much of the recent work in NLP has focused on sentiment analysis. This means the actual meaning on the a statement not the specific words. For example by identifying positive and negative statements.
In the conclusion the authors claim that great progress have been made in NLP the last five years and that the use of machine learning is critical for its success. Major financial interests exists in further development of NLP.
% Niklas%
\subsection{Niklas Fasth}
In paper \cite{ANN} the authors propose a deep neural network instead of a conventional Artificial Neural Network (ANN). With a conventional ANN only the information provided to the network is considered when a result is decided, however, with a Deep Neural Network (DNN) also the relations between the data is considered. To process the input data for the DNN the authors used NLP and two functions in particular, Part of speech (POS) tagging and Entity Recognition. POS tagging is used to classify the words from a sentence as nouns and verbs and so on. Entity Recognition is used to identify entities like persons, locations and time in the sentence.  The results from this papers was an application which took a document with data for the DNN to identify and classify the words at different layers and then when asked a question being able to answer accurately within the context of the document.

Authors in paper \cite{RestrictedDomain} introduced the problems of modern search engines when it comes to answering questions. Modern search engines often answers a question by listing a number of websites that might contain the answer. Then the user is required to analyze each website themselves to find the correct answer. Instead the paper propose a Question Answering System (QAS) that uses any search engine to find the top 10 pages related to the question asked and extracts the relevant information from these pages and merge the data into one text file. This text file is later used to extract the correct answer based on the question using one of three algorithms. This paper also identifies three different domains for a QAS. These are Closed Domain QA system, Open Domain QA system and Restricted Domain QA system. A Closed Domain QA system can answer questions within a specific subject. Open Domain QA Systems can find answers more broadly where a large collection of information.  The Restricted Domain QA system is a mix of both open and closed where a more accurate answer is easier than with an Open Domain QA system. This paper created their own dataset from books and web pages related to agriculture. 

The paper \cite{AutomaticQA} proposed a Support Vector Machine (SVM) as a baseline for a automatic question answering system. The authors also introduced high-frequency words and dependency relations additions to their baseline system for improved performance. Three components were identified in a QA system, question understanding, answer extraction and answer presentation.  Question understanding can also be divided into a number of stages where question classification has shown to heavily impact the accuracy of the answer. 
